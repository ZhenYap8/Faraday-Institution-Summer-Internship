{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script with ARD",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc8JiodOjZDg"
      },
      "source": [
        "1: To mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iY5BiiGGukB",
        "outputId": "d4d24617-8b66-4793-cd86-ee78cf071d0a"
      },
      "source": [
        "%reset -f\n",
        "from google.colab import drive \n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvC3COILicVO"
      },
      "source": [
        "2: To install category_encoders - for pre-processing, not really needed as the code is commented out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdjL6I1RG2g5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d024e580-c30a-4c83-ef1f-31aae16b3d85"
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.19.5)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvrjgKLpIvNq"
      },
      "source": [
        "3: The function below is used to get the data from the files using the pandas library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIUj1H_w_9pz"
      },
      "source": [
        "import pandas as pd\n",
        "from category_encoders import OrdinalEncoder  # convert categorical features, if non it won't change the data\n",
        "\n",
        "def get_values_df(filename, CapType, print_data_bool):\n",
        "    df = pd.read_csv(filename, encoding= 'unicode_escape')\n",
        "    df.to_csv(filename, header=[\"Weight\", \"Thickness\", \"Porosity%\", CapType], index=False)\n",
        "    df.to_csv(filename, header=[\"Weight\", \"Thickness\", \"Porosity%\", CapType], index=False)\n",
        "\n",
        "    if print_data_bool == 'yes':\n",
        "        print(df)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RvoCrTmuCo"
      },
      "source": [
        "4: Splits df into X and Y "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AK81vTbmdOx"
      },
      "source": [
        "def get_values_XY(df, CapType):\n",
        "    df.dropna(subset = [CapType], inplace=True)\n",
        "    y_df=df[df.columns[column_of_prediction]].to_frame() # specify output\n",
        "    X_df=df[df.columns[0:column_of_prediction]] # anything except the output is regressor\n",
        "    categorical_features = [col for col in X_df.columns if X_df[col].dtype == 'object']\n",
        "    # encoder = OrdinalEncoder(cols=categorical_features).fit(X_df)\n",
        "    # X_df=encoder.transform(X_df)\n",
        "\n",
        "    return X_df, y_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYqfFXH2hraF"
      },
      "source": [
        "5: PCA below, first need to install PCA factor map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb7pWeW4blsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea290fe-ca48-4e10-b7d0-7691f183925c"
      },
      "source": [
        "pip install -i https://test.pypi.org/simple/ variable-factor-map-Huy-Bui==0.0.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: variable-factor-map-Huy-Bui==0.0.3 in /usr/local/lib/python3.7/dist-packages (0.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSiAtosxAS8E"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from variable_factor_map import pca_map \n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "def PCA_function(X_df,show_bool, print_bool):\n",
        "    pca = PCA(n_components=2, svd_solver='full')\n",
        "    pca.fit(X_df)\n",
        "    pca_values = pca.components_\n",
        "    X_df_PCA = pca.transform(X_df)\n",
        "    exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "    px.area(\n",
        "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
        "    y=exp_var_cumul,\n",
        "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n",
        "    )\n",
        "\n",
        "    if print_bool == 'yes':\n",
        "        print('PCA explained variance ratio:', pca.explained_variance_ratio_)\n",
        "        print('PCA singular values:', pca.singular_values_)\n",
        "        print('PCA data shape: ', X_df_PCA.shape)\n",
        "        print('Initial data shape:', X_df.shape)\n",
        "        print('PCA components:', pca.components_, '\\n')\n",
        "        \n",
        "\n",
        "    if show_bool == 'yes':\n",
        "        pca_map(X_df, figsize=(10,10), sup='WMG Data', print_values= False)\n",
        "        plt.show()\n",
        "\n",
        "    return X_df_PCA, pca.explained_variance_ratio_, pca.singular_values_, pca.components_\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urP4AEmwhomI"
      },
      "source": [
        "6: Models and parameter grids below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG5kc4pI471f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.datasets import make_friedman2\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel, ExpSineSquared\n",
        "from sklearn.linear_model import ARDRegression\n",
        "\n",
        "#Gradient Boosting\n",
        "GB = GradientBoostingRegressor()\n",
        "param_grid_GB = {'n_estimators': [10, 100, 500],\n",
        "              'learning_rate': [0.1, 0.05, 0.02],\n",
        "              'max_depth': [4],\n",
        "              'max_features': [\"log2\", \"sqrt\"],\n",
        "              'criterion': [\"friedman_mse\", \"mae\"],\n",
        "              'min_samples_split': [3],\n",
        "              'min_samples_leaf': [3],\n",
        "              'max_features': [1.0]}\n",
        "\n",
        "#SVR\n",
        "SVR_model = SVR(kernel = 'rbf')\n",
        "# param_grid_SVR_model={'kernel' : ('linear', 'poly', 'rbf', 'sigmoid'),\"C\": [1e0, 1e1, 1e2, 1e3, 1e4],\n",
        "#                 \"gamma\": np.logspace(-2, 3, 10),'degree' : [3,8]}\n",
        "param_grid_SVR_model={\"C\": [1e0, 1e1, 1e2, 1e3, 1e4],\n",
        "                \"gamma\": np.logspace(-2, 3, 10),'degree' : [3,8]}\n",
        "\n",
        "\n",
        "#MLP - NN\n",
        "MLP = MLPRegressor(alpha = 0.06, solver = \"lbfgs\", hidden_layer_sizes=3, learning_rate_init=0.1)\n",
        "param_grid_MLP = {'hidden_layer_sizes': [i for i in range(1,5)],\n",
        "                  'activation': [\"relu\"],\n",
        "                  'solver': [\"lbfgs\", \"sgd\", \"adam\"],\n",
        "                  'learning_rate': ['constant'],\n",
        "                  'learning_rate_init': [i for i in [0.01,0.03,0.06,0.1]],\n",
        "                  'power_t': [0.5],\n",
        "                  'alpha': [i for i in [0.0001, 0.001,0.01]], #0.00001, 0.0001, 0.001, 0.01\n",
        "                  #0.0001 was used for C20\n",
        "                  'max_iter': [1000000],\n",
        "                  'early_stopping': [False],\n",
        "                  'warm_start': [False]}\n",
        "\n",
        "#Gaussian Process Regressor\n",
        "GPR = GaussianProcessRegressor()            \n",
        "param_grid_GPR = [{\"alpha\":  [1e0, 1e-1, 1e-2, 1e-3],\"kernel\": [RBF(l) for l in np.logspace(-1, 1, 2)]\n",
        "                   }, {\"alpha\":  [1e0, 1e-1, 1e-2, 1e-3],\"kernel\": [DotProduct(sigma_0) for sigma_0 in np.logspace(-1, 1, 2)]},\n",
        "                   {\"alpha\":  [1e0, 1e-1, 1e-2, 1e-3]}]\n",
        "# \"kernel\": [ExpSineSquared(l, p)+0.001*kernels.Identity()\n",
        "#                          for l in np.logspace(-2, 2, 10)\n",
        "#                          for p in np.logspace(0, 2, 10)]\n",
        "\n",
        "ARD = ARDRegression()\n",
        "param_grid_ARD = [{'alpha_1': [1e-1, 1e-2, 1e-3, 1e-5]}, {'alpha_2': [1e-1, 1e-2, 1e-3, 1e-5]}]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euO3jC_KC7AQ"
      },
      "source": [
        "7. To save estimators, parameters and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAuJGDl1CWV9"
      },
      "source": [
        "import csv\n",
        "def save_estimators_parameters_results(AOrC, CapType, C_Rate, index_filename, model_name_list, index_model, PCA_bool, all_parameters, all_estimators, results):\n",
        "    with open(\n",
        "            r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_' + AOrC + '_' + CapType + '_parameters_' +\n",
        "            C_Rate[\n",
        "                index_filename] + '_' + model_name_list[index_model] + '_' + PCA_bool + '.csv',\n",
        "            'w') as f:\n",
        "        for key in all_parameters.keys():\n",
        "            f.write(\"%s, %s\\n\" % (key, all_parameters[key]))\n",
        "    with open(\n",
        "            r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_' + AOrC + '_' + CapType + '_estimators_' +\n",
        "            C_Rate[\n",
        "                index_filename] + '_' + model_name_list[index_model] + '_' + PCA_bool + '.csv',\n",
        "            'w') as f:\n",
        "        for key in all_parameters.keys():\n",
        "            f.write(\"%s, %s\\n\" % (key, all_estimators[key]))\n",
        "    \n",
        "    np.savetxt(\n",
        "        r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_' + AOrC + '_' + CapType + '_results_' +\n",
        "        C_Rate[index_filename] + '_' + model_name_list[index_model] + '_' + PCA_bool + '.csv',\n",
        "        results,\n",
        "        delimiter=\",\")  # uncomment these two lines if the results for the individual C_rates are needed in individual files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDEPtYJeZJs7"
      },
      "source": [
        "8. Functions for feature importance for automatic relevance determination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFrUBhV3WuHG"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "def get_plots_ARD_coefs(estimator_, n_features):   \n",
        "      relevant_features = np.arange(0,n_features)\n",
        "      plt.figure(figsize=(6, 5))\n",
        "      plt.title(\"Weights of the model\")\n",
        "      plt.plot(estimator_.coef_, color='darkblue', linestyle='-', linewidth=2,\n",
        "         label=\"ARD estimate\")\n",
        "      plt.xlabel(\"Features\")\n",
        "      plt.ylabel(\"Values of the weights\")\n",
        "      plt.legend(loc=1)\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(6, 5))\n",
        "      plt.title(\"Histogram of the weights\")\n",
        "      plt.hist(estimator_.coef_, bins=n_features, color='navy', log=True)\n",
        "      plt.scatter(estimator_.coef_[relevant_features], np.full(len(relevant_features), 5.),\n",
        "            color='gold', marker='o', label=\"Relevant features\")\n",
        "      plt.ylabel(\"Features\")\n",
        "      plt.xlabel(\"Values of the weights\")\n",
        "      plt.legend(loc=1)\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(6, 5))\n",
        "      plt.title(\"Marginal log-likelihood\")\n",
        "      plt.plot(estimator_.scores_, color='navy', linewidth=2)\n",
        "      plt.ylabel(\"Score\")\n",
        "      plt.xlabel(\"Iterations\") \n",
        "      plt.show()    \n",
        "\n",
        "def get_FI(regr, X_df, FI_plot_bool):    \n",
        "    if model == GB: \n",
        "      importance = regr.best_estimator_.feature_importances_\n",
        "      if print_bool == 'yes':\n",
        "        for i,v in enumerate(importance):\n",
        "          print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "      pyplot.bar([x for x in range(len(importance))], importance)\n",
        "      if FI_plot_bool == 'yes':  \n",
        "        pyplot.show()\n",
        "    if model == SVR_model:\n",
        "      importance = regr.best_estimator_.dual_coef_\n",
        "    if model == GPR:\n",
        "      importance = regr.best_estimator_.alpha_\n",
        "    if model == ARD:\n",
        "      importance = regr.best_estimator_.coef_ \n",
        "      n_features = X_df.shape[1]-1\n",
        "      if FI_plot_bool == 'yes':\n",
        "          get_plots_ARD_coefs(regr.best_estimator_, n_features)\n",
        "    \n",
        "    if print_bool == 'yes':\n",
        "        print('Feature importance: \\n', importance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpRNhJshkkm"
      },
      "source": [
        "9. Functions used for training  below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQfLHdoQhef3"
      },
      "source": [
        "from IPython.display import display_html\n",
        "\n",
        "# apply the maximum absolute scaling in Pandas using the .abs() and .max() methods\n",
        "def maximum_absolute_scaling(df):\n",
        "    # copy the dataframe\n",
        "    df_scaled = df.copy()\n",
        "    # apply maximum absolute scaling\n",
        "    for column in df_scaled.columns:\n",
        "        df_scaled[column] = df_scaled[column] / df_scaled[column].abs().max()\n",
        "    return df_scaled\n",
        "\n",
        "def display_side_by_side(*args):\n",
        "    html_str=''\n",
        "    for df in args:\n",
        "        html_str+=df.to_html()\n",
        "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
        "\n",
        "def train(X_df, y_df, model, param_grid_in_use, print_bool, FI_bool, FI_plot_bool):\n",
        "    \n",
        "    _GS = GridSearchCV(model, param_grid=param_grid_in_use, n_jobs = -1)\n",
        "\n",
        "    X, y = make_regression(n_samples=200, random_state=0)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, random_state=0)\n",
        "    regr = _GS.fit(X_train, y_train.values.ravel())\n",
        "    # regr = model.fit(X_train, y_train.values.ravel()) # use this when not using gridsearch\n",
        "\n",
        "    # regr.predict(New_data) #for prediction on data outside dataset\n",
        "    \n",
        "    # y_pred = pd.DataFrame(regr.predict(X_test), columns=['pred'], index=X_test.index) # use this when not using gridsearch\n",
        "    y_pred = pd.DataFrame(regr.predict(X_test), columns=['pred'])\n",
        "\n",
        "\n",
        "    # comment out to make reading easier\n",
        "    # display_side_by_side(y_pred, y_test)\n",
        "    error = y_pred.values-y_test.values\n",
        "    if print_bool == 'yes':\n",
        "      print('regr score is', regr.score(X_test, y_test))\n",
        "      print('RMSE', np.mean(np.absolute(error)))\n",
        "\n",
        "    if FI_bool == 'yes':\n",
        "      get_FI(regr, X_df, FI_plot_bool)\n",
        "\n",
        "    return regr.score(X_test, y_test), np.mean(np.absolute(error)), regr.best_score_, regr.best_estimator_, regr.best_params_\n",
        "    # return regr.score(X_test, y_test), np.mean(np.absolute(error)) #use this when not using grid search\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i50kQ_f-yPTv"
      },
      "source": [
        "10. The function below is to plot all the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxA9FY7ZuO5A"
      },
      "source": [
        "from matplotlib.patches import Patch\n",
        "\n",
        "def get_plots(AOrC, CapType, result_name, C_Rate, model_name_list, PCA_bool, analytics, stds, save_plot_bool):\n",
        "    color_list = ['red', 'green', 'blue', 'black']\n",
        "    x_pos = np.arange(len(result_name))\n",
        "    # Build the plot\n",
        "    fig, axs = plt.subplots(len(C_Rate))\n",
        "    # fig.suptitle('All C Rates Results', fontsize = 20)\n",
        "    fig.subplots_adjust(top=0.8, hspace=0.5)\n",
        "    for i in range(len(C_Rate)):\n",
        "        for j in range(len(model_name_list)):\n",
        "            axs[i].bar(x_pos*len(model_name_list)+j, analytics[i,j], yerr=stds[i,j], align='center', alpha=1/len(model_name_list), ecolor='black', color =color_list[j] , capsize=10, label = C_Rate[i])\n",
        "        axs[i].set_ylabel('Results', fontsize = 5)\n",
        "        axs[i].set_xticks(x_pos, result_name)\n",
        "        axs[i].set_xlabel(', '.join(map(str, result_name)), fontsize=10)\n",
        "        axs[i].set_title('Results of models on I_O_' + AOrC + '_' + CapType + '_plots_' + PCA_bool +'_'+ C_Rate[i], fontsize = 15)\n",
        "        axs[i].yaxis.grid(True)\n",
        "\n",
        "    plt.subplots_adjust(left=0.1,\n",
        "                    bottom=1.0, \n",
        "                    right=0.9, \n",
        "                    top=7.0, \n",
        "                    wspace=0.4, \n",
        "                    hspace=0.4)\n",
        "\n",
        "    # map names to colors\n",
        "    cmap = dict(zip(model_name_list, color_list))\n",
        "    # create the rectangles for the legend\n",
        "    patches = [Patch(color=v, label=k) for k, v in cmap.items()]\n",
        "    # add the legend\n",
        "    plt.legend(title='Type of Model', labels=model_name_list, handles=patches, bbox_to_anchor=(1.04, 0.5), loc='center left', borderaxespad=0, fontsize=15, frameon=False)\n",
        "\n",
        "    # Save the figure and show\n",
        "    plt.tight_layout(pad=100.0) \n",
        "    plt.show()\n",
        "    if save_plot_bool == 'yes':\n",
        "        plt.savefig(r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_' + AOrC + '_' + CapType + '_plots_' + '_' + model_name_list[index_model] + '_'+ PCA_bool +'_'.join(result_name) + '_'+ PCA_bool + '.png', dpi=600)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm5R6xZesoL-"
      },
      "source": [
        "11: Initialise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbRQ2QCaodIW"
      },
      "source": [
        "# filename = r'/gdrive/My Drive/FUSE data/Experimental-Dataset.csv'\n",
        "C_Rate = [\"C20\", \"C5\", \"C2\", \"1C\", \"2C\", \"5C\", \"10C\"]\n",
        "CapType_list = [\"Cap\", \"VCap\", \"GCap\"]\n",
        "PCA_bool_list = [\"non-PCA\", \"PCA\"]\n",
        "AOrC_list = [\"Anode\", \"Cathode\"]\n",
        "AOrC = AOrC_list[0]\n",
        "CapType = CapType_list[0]\n",
        "column_of_prediction = 3\n",
        "# filename = r'/gdrive/My Drive/FUSE data/I_O_'+AOrC+'_'+CapType+'/I_O_'+AOrC+'_'+CapType+'_'+C_Rate[index_filename]+'.csv'\n",
        "\n",
        "#options for model are GB, SVR_model, GPR, MLP\n",
        "model_list = [ARD, GB, SVR_model, GPR]\n",
        "model_name_list = ['ARD', 'GB', 'SVR_model', 'GPR']\n",
        "param_grid_list = [param_grid_ARD, param_grid_GB, param_grid_SVR_model, param_grid_GPR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAP2vVnYrIRB"
      },
      "source": [
        "12: Add header to file names - this has to be run twice due to df.to_csv function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76O0HY_QqxWI"
      },
      "source": [
        "for index_AOrC in range(len(AOrC_list)):\n",
        "    for index_CapType in range(len(CapType_list)):\n",
        "            for index_filename in range(len(C_Rate)):\n",
        "                print_data_bool = 'no' #print data bool is set to no \n",
        "                filename = r'/gdrive/My Drive/FUSE data/I_O_'+AOrC_list[index_AOrC]+'_'+CapType_list[index_CapType]+'/I_O_'+AOrC_list[index_AOrC]+'_'+CapType_list[index_CapType]+'_'+C_Rate[index_filename]+'.csv'\n",
        "                # print(filename)\n",
        "                #get values\n",
        "                df = get_values_df(filename, CapType_list[index_CapType], print_data_bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4uaSpNlsgMO"
      },
      "source": [
        "for index_AOrC in range(len(AOrC_list)):\n",
        "    for index_CapType in range(len(CapType_list)):\n",
        "            for index_filename in range(len(C_Rate)):\n",
        "                print_data_bool = 'no' #print data bool is set to no \n",
        "                filename = r'/gdrive/My Drive/FUSE data/I_O_'+AOrC_list[index_AOrC]+'_'+CapType_list[index_CapType]+'/I_O_'+AOrC_list[index_AOrC]+'_'+CapType_list[index_CapType]+'_'+C_Rate[index_filename]+'.csv'\n",
        "                # print(filename)\n",
        "                #get values\n",
        "                df = get_values_df(filename, CapType_list[index_CapType], print_data_bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6USKgfhhiRP"
      },
      "source": [
        "13: Main run is below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQAyzeqhjd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "8a7c2b8c-fdfa-4d30-bbea-45cc268bf0f2"
      },
      "source": [
        "import time\n",
        "import csv\n",
        "import statistics as st\n",
        "import numpy as np\n",
        "for index_AOrC in range(len(AOrC_list)):\n",
        "    for index_PCA_bool in range(len(PCA_bool_list)):\n",
        "        for index_CapType in range(len(CapType_list)):\n",
        "            compiled_results = []\n",
        "            result_name = ['R^2', 'RMSE', 'Time']\n",
        "            analytics = np.zeros((len(C_Rate), len(model_name_list), len(result_name)))\n",
        "            stds = np.zeros((len(C_Rate), len(model_name_list), len(result_name)))\n",
        "            for index_filename in range(len(C_Rate)):\n",
        "                for index_model in range(len(model_name_list)):\n",
        "\n",
        "                    # option to print everything out\n",
        "                    print_bool = 'no'\n",
        "                    # option to print data\n",
        "                    print_data_bool = 'no'\n",
        "\n",
        "                    # option to use PCA or non-PCA data\n",
        "                    PCA_bool = PCA_bool_list[index_PCA_bool]\n",
        "                    # option to show the PCA map\n",
        "                    show_PCA_map_bool = 'no'\n",
        "                    # option to scale features\n",
        "                    scale_bool = 'no'\n",
        "                    \n",
        "                    # option to save the final results\n",
        "                    save_estimators_parameters_results_bool = 'no'\n",
        "                    # option to save the final results all compiled into one file\n",
        "                    save_compiled_results_bool = 'yes'\n",
        "                    # option to save the final results plot - this option has issues, need to manually save plots\n",
        "                    save_plot_bool = 'no'\n",
        "                    \n",
        "                    # option to print feature importance plot\n",
        "                    FI_bool = 'no'\n",
        "                    # option to show FI plots\n",
        "                    FI_plot_bool = 'no'\n",
        "\n",
        "                    # initialise lists and choose model\n",
        "                    scores = []\n",
        "                    RMSEs = []\n",
        "                    all_parameters = {}\n",
        "                    all_estimators = {}\n",
        "                    times = []\n",
        "                    model = model_list[index_model]\n",
        "                    model_name = model_name_list[index_model]\n",
        "                    param_grid_in_use = param_grid_list[index_model]\n",
        "\n",
        "                    #printing filename to check progress\n",
        "                    print('\\nI_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '_' + C_Rate[index_filename] + '_' + model_name_list[\n",
        "                        index_model] + '_' + PCA_bool)\n",
        "                    filename = r'/gdrive/My Drive/FUSE data/I_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '/I_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '_' + \\\n",
        "                               C_Rate[index_filename] + '.csv'\n",
        "                    # print(filename)\n",
        "\n",
        "                    # get values\n",
        "                    df = get_values_df(filename, CapType_list[index_CapType], print_data_bool)\n",
        "                    X_df, y_df = get_values_XY(df, CapType_list[index_CapType])\n",
        "\n",
        "                    # get only numerical values and call the maximum_absolute_scaling function, currently not used\n",
        "                    if scale_bool == 'yes':\n",
        "                        X_df = maximum_absolute_scaling(X_df)\n",
        "\n",
        "                    # run PCA\n",
        "                    X_df_PCA, PCA_explained_variance_ratio_, PCA_singular_values_, PCA_components_ = PCA_function(X_df, show_PCA_map_bool, print_bool)\n",
        "\n",
        "                    # to print parameters and group them in an array to print the learning curve later\n",
        "                    epochs = 3\n",
        "                    for i in range(epochs):\n",
        "\n",
        "                        print('Run:', i + 1)\n",
        "                        #To assign PCA or non-PCA X to train\n",
        "                        if PCA_bool == 'PCA':\n",
        "                            X_df_dummy = X_df_PCA\n",
        "                        else:\n",
        "                            X_df_dummy = X_df\n",
        "\n",
        "                        #Start training    \n",
        "                        start = time.time()\n",
        "                        score, error, best_score, best_estimator, best_params = train(X_df_dummy, y_df, model, param_grid_in_use, print_bool, FI_bool, FI_plot_bool)\n",
        "                        # score, error = train(X_df_PCA, y_df) #for use when not using grid search\n",
        "                        stop = time.time()\n",
        "                        print(f\"Training time: {stop - start}s\")\n",
        "                        times.append(stop - start)\n",
        "\n",
        "                        # storing results from multiple runs\n",
        "                        scores.append(score) #R^2 scores\n",
        "                        RMSEs.append(error) #RMSE scores\n",
        "                        all_estimators[str(i)] = best_estimator  # comment out when not using grid search\n",
        "                        all_parameters[str(i)] = best_params  # comment out when not using grid search\n",
        "\n",
        "                    # storing mean and stdev of results\n",
        "                    compiled_results.append([st.mean(scores), st.stdev(scores), st.mean(RMSEs), st.stdev(RMSEs), st.mean(times),st.stdev(times)]) #for the final table\n",
        "                    results = np.asarray([scores, RMSEs, times])  # individual file results\n",
        "                    analytics[index_filename, index_model] = [st.mean(scores), st.mean(RMSEs), st.mean(times)] #to store all mean values in array\n",
        "                    stds[index_filename, index_model] = [st.stdev(scores), st.stdev(RMSEs), st.stdev(times)] #to store all std values in array\n",
        "                    if print_bool == 'yes':\n",
        "                        print(results)\n",
        "\n",
        "                    #to write the data R^2, RMSE and best estimators, parameters and results parameters to .csv files, only change save_estimators&parameters&results_bool to 'yes' if there is space to store everything\n",
        "                    if save_estimators_parameters_results_bool == 'yes':\n",
        "                        save_estimators_parameters_results(AOrC_list[index_AOrC], CapType_list[index_CapType], C_Rate, index_filename, model_name_list, index_model, PCA_bool, all_parameters, all_estimators, results)\n",
        "                        print('Saved estimators, parameters and results for '+model_name_list[index_model]+' '+PCA_bool+' run on '+filename)\n",
        "\n",
        "            # the time plot is separated from the other result metrics\n",
        "            get_plots(AOrC_list[index_AOrC], CapType_list[index_CapType], result_name[0:-1], C_Rate, model_name_list, PCA_bool, analytics[:, :, 0:-1], stds[:, :, 0:-1],\n",
        "                      save_plot_bool)\n",
        "            get_plots(AOrC_list[index_AOrC], CapType_list[index_CapType], [result_name[-1]], C_Rate, model_name_list, PCA_bool, analytics[:, :, -1], stds[:, :, -1],\n",
        "                      save_plot_bool)\n",
        "\n",
        "            #save compiled results\n",
        "            compiled_results_array = np.asarray(compiled_results)\n",
        "            if save_compiled_results_bool == 'yes':\n",
        "                np.savetxt(\n",
        "                    r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '/results/I_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '_Compiled_Results_' + PCA_bool + '.csv', compiled_results_array, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I_O_Anode_Cap_C20_ARD_non-PCA\n",
            "Run: 1\n",
            "Training time: 1.2765576839447021s\n",
            "Run: 2\n",
            "Training time: 0.27225184440612793s\n",
            "Run: 3\n",
            "Training time: 0.2829318046569824s\n",
            "\n",
            "I_O_Anode_Cap_C20_GB_non-PCA\n",
            "Run: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-158-1895e3836724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0;31m#Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df_dummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_in_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_bool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFI_bool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFI_plot_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                         \u001b[0;31m# score, error = train(X_df_PCA, y_df) #for use when not using grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-ce29eb78d3cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_df, y_df, model, param_grid_in_use, print_bool, FI_bool, FI_plot_bool)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_GS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# regr = model.fit(X_train, y_train.values.ravel()) # use this when not using gridsearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq5fCVpj_lgP"
      },
      "source": [
        "This final section allows you to get the plots again. It make it easier to change the plot settings and not needing to run the training section again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw9Utn0fdmMe"
      },
      "source": [
        "save_plot_bool = 'no'\n",
        "get_plots(result_name[0:-1], C_Rate, model_name_list, PCA_bool, analytics[:,:,0:-1], stds[:,:,0:-1], save_plot_bool)\n",
        "get_plots([result_name[-1]], C_Rate, model_name_list, PCA_bool, analytics[:,:,-1], stds[:,:,-1], save_plot_bool)\n",
        "\n",
        "save_compiled_results_bool = 'no'\n",
        "if save_compiled_results_bool == 'yes':\n",
        "    np.savetxt(r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_'+ AOrC + '_' + CapType + '_Compiled_Results_' + PCA_bool + '.csv', compiled_results_array, delimiter=\",\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}